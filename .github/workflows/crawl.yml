name: Daily Event Crawl

on:
  schedule:
    # Run daily at midnight EST (05:00 UTC)
    - cron: '0 5 * * *'
  workflow_dispatch:
    # Allow manual trigger from GitHub UI
    inputs:
      source:
        description: 'Specific source to crawl (leave empty for all)'
        required: false
        default: ''

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: crawlers/requirements.txt

      - name: Install dependencies
        working-directory: crawlers
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright browsers
        working-directory: crawlers
        run: |
          playwright install chromium
          playwright install-deps chromium

      - name: Create config file
        working-directory: crawlers
        run: |
          cat > .env << EOF
          SUPABASE_URL=${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY=${{ secrets.SUPABASE_SERVICE_KEY }}
          ANTHROPIC_API_KEY=${{ secrets.ANTHROPIC_API_KEY }}
          TICKETMASTER_API_KEY=${{ secrets.TICKETMASTER_API_KEY }}
          EOF

      - name: Run crawler
        working-directory: crawlers
        run: |
          if [ -n "${{ github.event.inputs.source }}" ]; then
            echo "Running crawler for: ${{ github.event.inputs.source }}"
            python main.py --source "${{ github.event.inputs.source }}"
          else
            echo "Running all crawlers"
            python main.py
          fi

      - name: Summary
        if: always()
        working-directory: crawlers
        run: |
          echo "## Crawl Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Completed at: $(date)" >> $GITHUB_STEP_SUMMARY
